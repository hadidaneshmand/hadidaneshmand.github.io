<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
    <link rel="stylesheet" href="https://hadidaneshmand.github.io/jemdoc.css" type="text/css">
    <title>Hadi Daneshmand</title>
    <table summary="Table for page layout." id="tlayout">
    <tbody><tr valign="top">
    <td id="layout-menu">
    <div class="menu-category">Outline</div>
    <div class="menu-item"><a href="https://hadidaneshmand.github.io/hadiCV.pdf">CV</a></div>
    <div class="menu-item"><a href="#research">Research</a></div>
    <div class="menu-item"><a href="#awards">Awards</a></div>
    <div class="menu-item"><a href="#teaching">Teaching</a></div>
    <div class="menu-item"><a href="#papers">Publications</a></div>
    <div class="menu-item"><a href="#talk">Talks</a></div>
    <div class="menu-item"><a href="#service"> Services </a></div> 
    <div class="menu-item"><a href="#mentorship">Mentorship</a></div>
     
    <td id="layout-content">
    <div id="toptitle">
    <h1>Hadi Daneshmand</h1>
    </div>
    <table class="imgtable"><tbody><tr><td>
    <img style='border:1px solid #7393B3' src="https://hadidaneshmand.github.io/dhadi.png" alt="" width="200px">&nbsp;</td>
    <td><p>Assistant Professor of Computer Science </p>
    <h3>Contact</h3> 
    
    <p>105 UVA Rice Hall<br>
    85 Engineer's Way, Charlottesville, VA 22903 <br>
    
    Email: dhadiATvirginiaDOTedu<br></p>
    </td></tr></tbody></table>
    <p><br>
    I am an Assistant Professor of Computer Science at the University of Virginia. My research focuses on the theoretical foundations of machine learning, with an emphasis on theoretical analysis for explainability and reliability of generative AI using tools from probability theory, applied mathematics, and mathematical physics.
        Prior to joining UVA, I was a <a href="https://fodsi.us/"> FODSI</a>  postdoctoral researcher, hosted by MIT and Boston University. Before that, I served as a postdoctoral researcher at Princeton University as well as INRIA Paris. I completed my PhD in computer science at ETH Zurich in 2020. I had the privilege of being advised early in my research training by Professors <a href="https://scholar.google.com/citations?user=6PJWcFEAAAAJ&hl=en"> Francis Bach</a> and <a href="https://scholar.google.com/citations?user=T3hAyLkAAAAJ&hl=en"> Thomas Hofmann</a>.  
    
    
    
     
    <a name="research"><h2> Research </h2></a>
    <div class="container">
        <h1>Foundations of Generative AI</h1>
    
        <div class="intro">
          In a world increasingly shaped by generative AI, it is crucial to understand
          the underlying mechanisms that govern these models in order to rigorously
          characterize their strengths and limitations. My research focuses on the theoretical foundations of generative AI:
            analyzing its mechanisms, formulating challenges for reliable and efficient data generation,
            and designing new generative methods that are mathematically tractable and provably reliable.
        </div>
    
        <!-- Section 1: Analyzable generative models -->
        <div class="section" id="section-generative-models">
          <div class="section-title">
            Analyzable Generative Models Using Mathematical Physics
          </div>
          <div class="section-content">
            <div class="section-gif">
              <!-- Replace with your GIF -->
              <img src="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/images/efs.gif?raw=true" alt="Generative models animation">
            </div>
            <div class="section-text">
              We use tools from mathematical physics to design novel generative methods
              that can be analyzed rigorously in asymptotic regimes. These generative methods
              provably generate data from the underlying distribution under
              weak assumptions without noise injection and neural networks;
              instead, it is implemented based on first-order optimization. The figure illustrates how interacting particles evolve to generate data from the Swiss roll dataset.
              <br>See <a href="#22">[22]</a>
            </div>
          </div>
        </div>
    
        <!-- Section 2: In-context learning & mechanistic analysis -->
        <div class="section" id="section-icl-mech">
          <div class="section-title">
            In-Context Learning and Mechanistic Analysis of Large Language Models
          </div>
          <div class="section-content">
            <div class="section-gif">
              <!-- Replace with your GIF -->
              <img src="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/images/icl.gif?raw=true" alt="In-context learning animation">
            </div>
            <div class="section-text">
                This line of work examines how large language models internally generate data. By analyzing their attention patterns and internal representations, we aim to uncover mechanistic explanations for how transformers perform tasks such as sorting, token alignment, reinforcement learning, and regression without changing their parameters. Our goal is to move beyond black-box performance and develop a mathematically grounded understanding of what these models compute and when they fail.
                The figure shows the attention heat map for token alignment across layers of a large language model.           
                <br> 
            See <a href="#23"> [23]</a>, <a href="#21"> [21]</a>, <a href="#20"> [20]</a> and  <a href="#19"> [19]</a>
            </div>
          </div>
        </div>
    
        <!-- Section 3: Markov chain analysis -->
        <div class="section" id="section-markov-chains">
          <div class="section-title">
            Markov Chain Analysis of Random Deep Neural Networks
          </div>
          <div class="section-content">
            <div class="section-gif">
              <!-- Replace with your GIF -->
              <img src="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/images/iso.jpg?raw=true" alt="Markov chain dynamics in deep nets"> 
              
            </div>
            <div class="section-text">
                The intermediate data representations in deep neural networks form a Markov chain at initialization. In this project, we apply tools from Markov chain theory to analyze this chain. This perspective provides a principled explanation of a key architectural component in deep neural networks and large language models: normalization layers. By establishing a stochastic stability analysis, we show that normalization inherently biases representations toward being whitened across layers. This finding helps explain why normalization plays a crucial role in improving optimization and training stability in foundation models. <br> 
                See papers <a href="#18"> [18]</a>, <a href="#17"> [17]</a>, <a href="#16"> [16]</a>, [14] and [13]
            </div>
          </div>
        </div>
    
        <!-- Section 4: Optimization challenges -->
        <div class="section" id="section-optimization">
          <div class="section-title">
            Optimization Challenges in Generative AI
          </div>
          <div class="section-content">
            <div class="section-gif">
              <!-- Replace with your GIF -->
             <img src="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/images/saddle.jpg?raw=true" alt="Optimization in generative AI"> 
            </div>
            <div class="section-text">
                Generative AI models are trained using highly non-convex objective functions that induce complex optimization dynamics. In this research direction, we investigate the fundamental optimization challenges that arise in generative AI. Generative adversarial networks, in particular, optimize a min–max objective using first-order methods. We introduce the notion of local optimization for min–max problems and prove that first-order dynamics can converge to stable attractors that are not locally optimal. This stands in sharp contrast to smooth optimization in supervised learning, where local optima are precisely the stable attractors of the dynamics.
            <br> See paper [11]
            </div>
          </div>
        </div>
      </div> 
        
    <a><h2> News </h2></a> 
        <ul><li>Warm welcome to <a href="https://scholar.google.com/citations?user=ePu-g_EAAAAJ&hl=en&oi=ao" target="_blank">Alireza Jafari</a>, the first member of my group.</li>
     <li> You are warmly invited to my contributed talk at the NeurIPS 2025 workshop on <a href="https://neurips.cc/virtual/2025/loc/san-diego/workshop/109581" target="_blank">Optimization for Machine Learning</a>, titled <a href="https://arxiv.org/abs/2507.08239" target="_blank">Data Generation without Function Estimation</a>, a joint work with <a href="https://ashkansoleymani.lids.mit.edu/" target="_blank">Ashkan Soleymani</a> from MIT.</li>
       <li> You are warmly invited to our poster presentation at NeurIPS25 on <a href="https://arxiv.org/pdf/2509.19702?">Linear Transformers Implicitly Discover
    Unified Numerical Algorithms</a>, a joint work with Patrick Lutz, Aditya Gangrade and Venkatesh Saligrama from Boston University.</li>
    <!--   <li> I am deeply honored and grateful to receive the <a href="https://cpal.cc/rising_stars_guidelines/\">Stanford CPAL Rising Star Award</a> </li>
            <li> I am developing a new course on "Neural Network: A Theory Lab" at UVA (<a href="https://www.cs.virginia.edu/~xay7te/courses/neuralnets/index.html">website</a>, <a href="https://hackmd.io/@hadidanesh/SJJYw3Lvke">notes</a>)</li>
      <li> Jiuqi Wang, Ethan Blaser and Shangtong Zhang and I are honored to receive the <a href="https://iclworkshop.github.io/">spotlight award at ICML24 in-context learning workshop</a> </li> -->
    </ul>
        
    <a name="awards"><h2>Awards</h2></a>
    <ul>
     <li><b> Research </b> </li>
    <ul>
        <li>Rising Star Award,<a href="https://cpal.cc/rising_stars_guidelines/\"> The Conference on Parsimony and Learning at Stanford University</a> </li>
        <li>  Spotlight award with <a href="https://leonardowjq.github.io/"> Jiuqi Wang</a>, <a href="https://blaserethan.github.io/"> Ethan Blaser </a> , and  <a href="https://shangtongzhang.github.io/"> Shangtong Zhang </a> for the publication [20] at the <a href="https://iclworkshop.github.io/">ICML In-context Learning workshop in 2024 </a> 
        </li>
        <li> Postdoc fellowship of the <i> Foundation of Data Science Insititute (FODSI) </i>. <b>Outputs:  </b> <a href="#17"> [17]</a>, <a href="#18"> [18] </a>, <a href="#19"> [19] </a>, <a href="#20"> [20] </a>, <a href="#21"> [21] </a>
           </li>
        <li> <a href="https://www.snf.ch/en/f6JZyI4uQ1mNeq3J/funding/discontinued-funding-schemes/early-postdoc-mobility"> Early Postdoc Mobility</a> grant from <a href="https://www.snf.ch/en">SNSF</a> for <i> Bridging the gap between local and global optimization in machine learning</i>, 20. <b>Outputs:</b> <a href="#10"> [10]</a>, <a href="#15"> [15]</a> and <a href="#16"> [16]</a> </li>
    <li> Best poster award in deep learning workshop of <a href="https://learning-systems.org"> Max Planck ETH center for learning systems</a> 2016.</li>
    </ul>
     </li>
        <li><b>Service</b></li>
        Reviewer awards for my service to
        <ul>
            <li>ICML 22 </li>
            <li> NeurIPS 20  </li>
            <li> ICML 19 </li> 
        </ul>
    
    </ul>
    <a name="teaching"><h2> Teaching </h2></a>
    <ul> 
        <li> Machine Learning, undergraduate coures, we developed a series of machine learning puzzles that encourage understanding of foundations of ML (uva page)</li>
        <li> 
    "Neural Networks: A Theory Lab" is a course designed to introduce the theoretical foundations of neural computing through hands-on, in-class coding exercises (<a href="https://www.cs.virginia.edu/~xay7te/courses/neuralnets/index.html">website</a>, <a href="https://hackmd.io/@hadidanesh/SJJYw3Lvke">notes</a>).</li>    
    </ul>
    <a name="papers"><h2> Publications (<a href="https://scholar.google.com/citations?user=roFM8XsAAAAJ&hl=en">Google Scholar</a>)</h2>
    <ul>
    <li> <a> Foundations of Generative AI </a> </li> 
    <br> 
    
    
    <ul>
        <li> <a name="23">[23]</a> <a href="https://arxiv.org/pdf/2509.19702?"> Linear Transformers Implicitly Discover Unified Numerical Algorithms</a>  with Patrick Lutz Aditya Gangrade and Venkatesh Saligrama.  NeurIPS 2025</li>
        <li> <a name="22">[22]</a>  <a href="https://arxiv.org/abs/2507.08239"> Data Generation without Function Estimation</a> with Ashkan Soleymani. NeurIPS25 workshop on Optimization for Machine Learning; <b>(selected for a contributed talk) </b></li>
        <li> <a name="21">[21]</a> Provable optimal transport with transformers: The essence of depth and prompt engineering, <a href="https://arxiv.org/pdf/2410.19931"> ArXiv 2024</a> (<a href="https://github.com/hadidaneshmand/ot-transformer">code</a>)</li>
        <li> <a name="20">[20]</a> Transformers Learn Temporal Difference Methods for In-Context Reinforcement Learning with Jiuqi Wang, Ethan Blaser, and Shangtong Zhang. <a href="https://openreview.net/pdf?id=mEqddgqf5w"> ICML workshop on In-context Learning</a> <b>(spotlight award)</b>  </li>
    <li>  <a name="19">[19]</a> Transformers learn to implement preconditioned gradient descent for in-context learning with Kwangjun Ahn, Xiang Cheng and Suvrit Sra. <a href="https://arxiv.org/pdf/2306.00297.pdf">NeurIPs 23</a> (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/raw/main/slides/ICL.key">slides</a>) </li>
    
    </ul>
    <li> <a> Bridging the gap between theory and practice in deep learning </a> (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/raw/main/slides/stanford.key">slides</a>) <!-- <div class="popup" onclick="myFunction()"> (<a href"">abstract</a>)<span class="popuptext" id="myPopup">Neural networks can implemient iterative algorithms across their layers. We prove that even random neural networks can implement simple algorithms. Then, we investigate the possiblity of learning gradient descent with transformers. </span></div> --> 
    </li>
    <br>
    
    <ul> 
        <li> <a name="18"> [18]</a> Towards Training Without Depth Limits: Batch Normalization Without Gradient Explosion with Alexandru Meterez, Amir Joudaki, Francesco Orabona, Alexander Immer, Gunnar Ratsch. <a href="https://arxiv.org/abs/2310.02012">ICLR 24</a>  </li>
        <li> <a name="17"> [17] </a> On the impact of activation and normalization in obtaining isometric embeddings at initialization with Amir Joudaki and Francis Bach. <a href="https://arxiv.org/pdf/2305.18399.pdf"> NeurIPs 23</a></li>
    <li> <a name="16"> [16]</a> On bridging the gap between mean field and finite width in deep random neural networks with batch normalization with Amir Joudaki and Francis Bach, <a href=""https://arxiv.org/abs/2205.13076> ICML 23</a> (<a href="https://github.com/ajoudaki/mean-field-normalization">code</a>,<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/posters/mean_field_poster_icml.pdf"> poster</a>)</li>
    <li> <a name="15"> [15]</a> Efficient displacement convex optimization with particle gradient descent with Jason D. Lee, and Chi Jin. <a href="https://arxiv.org/abs/2302.04753"> ICML 23</a> (<a href="https://github.com/hadidaneshmand/icml23_pgd">code</a>, <a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/posters/BN_NeurIPS20_poster.pdf">poster</a>) </li>
    </ul>
    
    
    <li> <a> Data representations in deep random neural networks </a> (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/raw/main/slides/mit-tea-talk.key">slides</a>)</li>
    <br> 
    <ul>
    <li> [14] Batch normalization orthogonalizes representations in deep random networks  with Amir Joudaki, and Francis Bach. <a href="https://openreview.net/pdf?id=_RSgXL8gNnx"> <b>(spotlight) </b> NeurIPS 21</a> (<a href="https://github.com/hadidaneshmand/batchnorm21">code</a>, <a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/posters/BN_poster_NeurIPS21.pdf">poster</a>)  </li> 
    
    <li> [13] Batch normalization provably avoids ranks collapse for randomly initialised deep networks with Jonas Kohler, Francis Bach, Thomas Hofmann, Aurelien Lucchi. <a href="https://proceedings.neurips.cc/paper/2020/file/d5ade38a2c9f6f073d69e1bc6b6e64c1-Paper.pdf">NeurIPS 20</a> (<a href="https://github.com/hadidaneshmand/bn_neurips20">code</a>, <a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/posters/BN_NeurIPS20_poster.pdf">poster</a>) </li>
    </ul>
    <li> <a>Learning dynamics in deep neural networks</a>
    <br> 
    <br>
    <ul> 
    <li> [12] Exponential convergence rates for batch normalization: The power of length-direction decoupling in non-convex optimization with Jonas Kohler, Aurelien Lucchi, Thomas Hofmann, Ming Zhou and Klaus Neymeyr. <a href="http://proceedings.mlr.press/v89/kohler19a/kohler19a.pdf">AISTATS 19</a> </li>
    </li>
    <li> [11] Local saddle point optimization: A curvature exploitation approach with Leonard Adolphs, Aurelien Lucchi and Thomas Hofmann. <a href="http://proceedings.mlr.press/v89/adolphs19a/adolphs19a.pdf">AISTATS 19</a> (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/posters/icml18_poster.pdf">poster</a>)</li> 
    
    <li> <a name="10"> [10]</a> Polynomial-time Sparse Measure Recovery: From Mean Field Theory to Algorithm Design with Francis Bach, <a href="https://arxiv.org/pdf/2204.07879.pdf"> arXiv 2022</a> </li>
    </ul>
    <li> <a> Modeling accelerated optimization methods by ordinary differential equations </a> 
    <br>
    <br>
    <ul> 
    <li> [9] Rethinking the Variational Interpretation of Accelerated Optimization Methods with Peiyuan Zhang, and Antonio Orvieto. <a href="https://proceedings.neurips.cc/paper/2021/file/788d986905533aba051261497ecffcbb-Paper.pdf">NeurIPS 21</a> (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/posters/neurips21.pdf">poster</a>)
    </li>
    <li> 
    [8] Revisiting the role of euler numerical integration on acceleration and stability in convex optimization with Peiyuan Zhang, Antonio Orvieto, Thomas Hofmann and Roy S Smith. <a href="http://proceedings.mlr.press/v130/zhang21m/zhang21m.pdf">AISTATS 18</a> (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/posters/aistats19.pdf">poster</a>)
    </li>
    </ul>
    <li> <a> Stochastic optimization for machine learning</a> </li>
    <br>
    <ul>
    <li> [7] Escaping saddles with stochastic gradients with  Jonas Kohler, Aurelien Lucchi and Thomas Hofmann. <a href="http://proceedings.mlr.press/v80/daneshmand18a/daneshmand18a.pdf"><b>(long presentation)</b> ICML 18</a>(<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/slides/icml18.pdf">slides</a>)</li>
    <li> [6] Adaptive newton method for empirical risk minimization to statistical accuracy with Aryan Mokhtari, Hadi Daneshmand, Aurelien Lucchi, Thomas Hofmann and Alejandro Ribeiro. <a href="https://proceedings.neurips.cc/paper/2016/file/9f62b8625f914a002496335037e9ad97-Paper.pdf"> NeurIPS 16</a> </li>
    <li> [5] Starting small-learning with adaptive sample sizes with Aurelien Lucchi and Thomas Hofmann. <a href="http://proceedings.mlr.press/v48/daneshmand16.pdf"> ICML 16 </a> </li> 
    </ul>
    <li> <a> Inference of graphs from diffusion processes</a> </li>
    <br> 
    <ul>
    <li> [4] Estimating diffusion network structures: Recovery conditions, sample complexity & soft-thresholding algorithm with  Manuel Gomez-Rodriguez, Le Song and Bernhard Schoelkopf.<a href="http://proceedings.mlr.press/v32/daneshmand14.pdf"> ICML 14 (<b>Recomended to JMLR</b>)</a> </li>
    <li> [3] Estimating diffusion networks: Recovery conditions, sample complexity & soft-thresholding algorithm with  Manuel Gomez-Rodriguez, Le Song and Bernhard Schoelkopf. <a href="https://www.jmlr.org/papers/volume17/14-430/14-430.pdf"> JMLR 16</a> </li>
    <li> [2] A Time-Aware Recommender System Based on Dependency Network of Items with Amin Javari, Seyed Ebrahim Abtahi and Mahdi Jalili.  <a href="https://ieeexplore.ieee.org/abstract/document/8213961"> The Computer Journal 14</a> </li> 
    <li> [1] Inferring causal molecular networks: empirical assessment through a community-based effort with Steven M Hill, Laura M Heise et al. Nature Methods</li>
    </ul>
    </ul>
    
    <!-- <script>// When the user clicks on div, open the popupfunction myFunction() {
      var popup = document.getElementById("myPopup");
      popup.classList.toggle("show");
    }</script>-->
    
    
    
    <a name="talk"><h2>Talks</h2></a>
<ul> 
    <li> Forthcoming Talk</li>
    <ul> 
        <li> "Data Generation without Function Estimation" will be presented at the NeurIPS 2025 Workshop on Optimization for Machine Learning.</li>
    </ul>
    <li> Past</li>
    <ul> 

    <li> "Learning to Compute" presented at Seminar Series "Youth in High-Dimensions", The Abdus Salam International Centre for Theoretical Physics, 2025</li>
    <li> "Understanding Test-time Inference in LLMs" presented at Conference on Parsimony and Learning, Stanford University, 2025</li>
    <li> Invited to the final presentation for Vienna Research Groups for Young Investigators Grant (1.6 million euro), Austria, 2024. My sincere thanks go to <a href="https://www.benjaminroth.net/"> Benjamin Roth <a> </a>and <a href="https://sebschu.com/"> Sebastian Schuster </a> for their excellent advice and help </li>
        <li> "What makes neural networks statistically powerful, and optimizable?" presented at Extra Seminar on Artificial Intelligence of University of Groningen and The University of Edinburgh 2024 (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/raw/main/slides/groningen.key">slides</a>) </li>
    <li> "Algorithmic View on Neural Information Processing". Mathematics, Information, and Computation Seminar, New York University 2023 (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/raw/main/slides/ICL.key">slides</a>) </li>
    <li>Beyond Theoretical Mean-field Neural Networks at ISL Colloquium, Stanford University, July 23 (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/raw/main/slides/stanford.key">slides</a>) </li>
    <li>Data representation in deep random neural networks at ML tea talks, MIT, March 23 (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/raw/main/slides/mit-tea-talk.key">slides</a>)</li>
    <li>The power of depth in random neural networks at Princeton University, April 22</li>
    <!-- <li> Representations in Random Deep Neural Networks at Winter Seminar Series, Sharif University 22(<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/slides/wss22.pdf">slides</a>)</li> -->
    <li> Batch normalization orthogonalizes representations in deep random neural networks, spotlight at NeurIPs 21 (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/slides/neurips21.pdf">slides</a>)</li>  
    <li> Representations in Random Deep Neural Networks at INRIA 21 (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/slides/inria.pdf">slides</a>) </li> 
    
    <li> Escaping Saddles with Stochastic Gradients at ICML 18 (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/slides/icml18.pdf">slides</a>)</li>
    </ul>
</ul> 
    <a name="service"><h2>Academic Services</h2></a>
    <ul> 
    <li> Area chair for 
   </li>
    <ul> <li> Conference on Neural Information Processing Systems 2023, 2024, and 2025  </li> 
     <li> International Conference on Machine Learning 2025</li>   
     </ul>
    <li> A member of the organizing team for   </li>
        <ul>
        <li>  Session chair at INFORMS/IOS 24</li>
        <li> Session chair at NeurIPS 23  </li>
        <li>  Reviewing talks for NeurIPS 23</li>
        <li> ICLR 24 Workshop on <i>Bridging the Gap Between Practice and Theory in Deep Learning</i> led by Jingzhao Zhang</li>
        <li> <a href="https://sites.google.com/view/optml/home"> TILOS & OPTML++ seminars at MIT, 2023</a> </li>
        </ul>  
  
    <li> Reviewer for ICML, NeurIPs, ICLR and AISTATS</li>
    <li> Journal reviewer for Data Mining and Knowledge Discovery, Neurocomputing, TPAMI, and TSIPN </li> 
    </ul>
    <h2> <a name="mentorship"> Research Group </a></h2> 
        <ul>
    <li> <a href="https://scholar.google.com/citations?user=ePu-g_EAAAAJ&hl=en&oi=ao"> Alireza Jafari</a>, PhD student at the University of Virginia  </li>
        </ul>
    <h2> <a name="mentorship"> Mentorship </a></h2> 
    I am privileged to work with excellent students who are potential future leaders of machine learning research. 
    <ul>
    <li> Amir Joudaki, Ph.D. student at ETH Zurich (20-24), Admitted to a postdoctoral position at Broad Institute </li>
    <li> Alexandru Meterez, Master's student at ETH (22-23), Joined Harvard University for PhD. </li> 
    <li> Flowers Alec Massimo, Master's student at ETH (23), Joined INVIDIA</li> 
    <li> Jonas Kohler, former Ph.D. student at ETH  (18-20), joined Meta</li> 
    <li> Antonio Orvieto, Ph.D. student at ETH Zurich (20-21) </li>
    <li> Peiyuan Zhang, Master's student at ETH (19-21), joined Yale University for Ph.D. </li>
    <li> Leonard Adolphs, Master's student at ETH (18-19), joined ETH Zurich for Ph.D. </li> 
    
    
    <li> Alexandre Bense, Master's student at ETH (22) </li> 
    <li> Alireza Amani, Intern at ETH (18), joined London Business School for Ph.D. </li>
        </ul>
        
    </p></td></tr></tbody></table></head></html>
    
    