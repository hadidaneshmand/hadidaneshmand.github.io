<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<link rel="stylesheet" href="https://hadidaneshmand.github.io/jemdoc.css" type="text/css">
<title>Hadi Daneshmand</title>
<table summary="Table for page layout." id="tlayout">
<tbody><tr valign="top">
<td id="layout-menu">
<div class="menu-category">Outline</div>
<div class="menu-item"><a href="https://hadidaneshmand.github.io/hadiCV.pdf">CV</a></div>
<div class="menu-item"><a href="#papers">Research</a></div>
<div class="menu-item"><a href="#awards">Awards</a></div>
<div class="menu-item"><a href="#talk">Talks</a></div>
<div class="menu-item"><a href="#service"> Services </a></div> 
<div class="menu-item"><a href="#mentorship">Mentorship</a></div>
 
<td id="layout-content">
<div id="toptitle">
<h1>Hadi Daneshmand</h1>
</div>
<table class="imgtable"><tbody><tr><td>
<img style='border:1px solid #7393B3' src="https://hadidaneshmand.github.io/dhadi.png" alt="" width="200px">&nbsp;</td>
</script>
<td><p>Postdoctoral researcher at Foundation of Data Science Institute hosted by <br>
 <ul style="list-style-type:none;"> 
 <li> MIT Laboratory for Information & Decision Systems </li>
 <li> Hariri Institute at Boston University </li>
<h3>Contact</h3>
<p>MIT D32-588<br>
Cambridge, USA<br>
dhadi at mit dot edu<br></p>
</td></tr></tbody></table>
<p><br>
I analyze the data representation in deep neural networks at <a href="https://fodsi.us/"> FODSI</a> hosted by 
<a href="https://optml.mit.edu/group.html">Suvrit Sra,</a>
<a href="https://francesco.orabona.com/"> Francesco Orabona,</a> and 
<a href="https://www.bu.edu/eng/profile/venkatesh-saligrama/"> Venkatesh Saligrama</a>. Before, I was an <a href="https://www.snf.ch/en">SNSF</a> postdoctoral researcher at Princeton University and <a href="https://www.di.ens.fr/~fbach/">Francis Bach</a>'s group at INRIA Paris. I accomplished  my Ph.D. in computer science under the supervision of <a href="http://www.da.inf.ethz.ch/people/ThomasHofmann">Thomas Hofmann</a> at ETH Zurich.

 
<a name="papers"><h2> Research Output (<a href="https://scholar.google.com/citations?user=roFM8XsAAAAJ&hl=en">Google Scholar</a>)</h2>
<ul style="list-style-type:none;"> 
<br>

<li> <a> Beyond a mean-field theory for neural networks </a> <!-- <div class="popup" onclick="myFunction()"> (<a href"">abstract</a>)<span class="popuptext" id="myPopup">Neural networks can implemient iterative algorithms across their layers. We prove that even random neural networks can implement simple algorithms. Then, we investigate the possiblity of learning gradient descent with transformers. </span></div> --> 
<br>
<br>
<ul> 
<li>  [1] Transformers learn to implement preconditioned gradient descent for in-context learning with Kwangjun Ahn, Xiang Cheng and Suvrit Sra. <a href="https://arxiv.org/pdf/2306.00297.pdf">arXiv 23</a> </li>
<li> [2] Batch normalization orthogonalizes representations in deep random networks  with Amir Joudaki, and Francis Bach. <a href="https://openreview.net/pdf?id=_RSgXL8gNnx"> <b>(spotlight) </b> NeurIPS 21</a> (<a href="https://github.com/hadidaneshmand/batchnorm21">code</a>, <a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/posters/BN_poster_NeurIPS21.pdf">poster</a>)  </li> 
<li> [3] On the impact of activation and normalization in obtaining isometric embeddings at initialization with Amir Joudaki and Francis Bach. <a href="https://arxiv.org/pdf/2305.18399.pdf"> arXiv 23</a></li>
<li> <a name="4"> [4]</a> On bridging the gap between mean field and finite width in deep random neural networks with batch normalization with Amir Joudaki and Francis Bach, <a href=""https://arxiv.org/abs/2205.13076> ICML 23</a> (<a href="https://github.com/ajoudaki/mean-field-normalization">code</a>,<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/posters/mean_field_poster_icml.pdf"> poster</a>)</li>
<li> [5] Batch normalization provably avoids ranks collapse for randomly initialised deep networks with Jonas Kohler, Francis Bach, Thomas Hofmann, Aurelien Lucchi. <a href="https://proceedings.neurips.cc/paper/2020/file/d5ade38a2c9f6f073d69e1bc6b6e64c1-Paper.pdf">NeurIPS 20</a> (<a href="https://github.com/hadidaneshmand/bn_neurips20">code</a>, <a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/posters/BN_NeurIPS20_poster.pdf">poster</a>) </li>
<li> <a name="6"> [6]</a> Efficient displacement convex optimization with particle gradient descent with Jason D. Lee, and Chi Jin. <a href="https://arxiv.org/abs/2302.04753"> ICML 23</a> (<a href="https://github.com/hadidaneshmand/icml23_pgd">code</a>, <a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/posters/BN_NeurIPS20_poster.pdf">poster</a>) </li>
</ul>
</li>
<li> <a>Non-convex optimization for neural networks</a>
<br> 
<br>
<ul> 
<li> [7] Exponential convergence rates for batch normalization: The power of length-direction decoupling in non-convex optimization with Jonas Kohler, Aurelien Lucchi, Thomas Hofmann, Ming Zhou and Klaus Neymeyr. <a href="http://proceedings.mlr.press/v89/kohler19a/kohler19a.pdf">AISTATS 19</a> </li>
</li>
<li> [8] Local saddle point optimization: A curvature exploitation approach with Leonard Adolphs, Aurelien Lucchi and Thomas Hofmann. <a href="http://proceedings.mlr.press/v89/adolphs19a/adolphs19a.pdf">AISTATS 19</a> (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/posters/icml18_poster.pdf">poster</a>)</li> 

<li> <a name="9"> [9]</a> Polynomial-time Sparse Measure Recovery: From Mean Field Theory to Algorithm Design with Francis Bach, <a href="https://arxiv.org/pdf/2204.07879.pdf"> arXiv 2022</a> </li>
</ul>
<li> <a> Modeling accelerated optimization methods by ordinary differential equations </a> 
<br>
<br>
<ul> 
<li> [10] Rethinking the Variational Interpretation of Accelerated Optimization Methods with Peiyuan Zhang, and Antonio Orvieto. <a href="https://proceedings.neurips.cc/paper/2021/file/788d986905533aba051261497ecffcbb-Paper.pdf">NeurIPS 21</a> (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/posters/neurips21.pdf">poster</a>)
</li>
<li> 
[11] Revisiting the role of euler numerical integration on acceleration and stability in convex optimization with Peiyuan Zhang, Antonio Orvieto, Thomas Hofmann and Roy S Smith. <a href="http://proceedings.mlr.press/v130/zhang21m/zhang21m.pdf">AISTATS 18</a> (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/posters/aistats19.pdf">poster</a>)
</li>
</ul>
<li> <a> Stochastic optimization for machine learning</a> </li>
<br>
<ul>
<li> [12] Escaping saddles with stochastic gradients with  Jonas Kohler, Aurelien Lucchi and Thomas Hofmann. <a href="http://proceedings.mlr.press/v80/daneshmand18a/daneshmand18a.pdf"><b>(long presentation)</b> ICML 18</a></li>
<li> [13] Adaptive newton method for empirical risk minimization to statistical accuracy with Aryan Mokhtari, Hadi Daneshmand, Aurelien Lucchi, Thomas Hofmann and Alejandro Ribeiro. <a href="https://proceedings.neurips.cc/paper/2016/file/9f62b8625f914a002496335037e9ad97-Paper.pdf"> NeurIPS 16</a> </li>
<li>Starting small-learning with adaptive sample sizes with Aurelien Lucchi and Thomas Hofmann. <a href="http://proceedings.mlr.press/v48/daneshmand16.pdf"> ICML 16 </a> </li> 
</ul>
<li> <a> Inference of graphs from diffusion processes</a> </li>
<br> 
<ul>
<li> [14] Estimating diffusion network structures: Recovery conditions, sample complexity & soft-thresholding algorithm with  Manuel Gomez-Rodriguez, Le Song and Bernhard Schoelkopf.<a href="http://proceedings.mlr.press/v32/daneshmand14.pdf"> ICML 14 (<b>Recomended to JMLR</b>)</a> </li>
<li> [15] Estimating diffusion networks: Recovery conditions, sample complexity & soft-thresholding algorithm with  Manuel Gomez-Rodriguez, Le Song and Bernhard Schoelkopf. <a href="https://www.jmlr.org/papers/volume17/14-430/14-430.pdf"> JMLR 16</a> </li>
<li> [16] A Time-Aware Recommender System Based on Dependency Network of Items with Amin Javari, Seyed Ebrahim Abtahi and Mahdi Jalili.  <a href="https://ieeexplore.ieee.org/abstract/document/8213961"> The Computer Journal 14</a> </li> 
<li> [17] Inferring causal molecular networks: empirical assessment through a community-based effort with Steven M Hill, Laura M Heise et al. Nature Methods</li>
</ul>
</ul>

<!-- <script>// When the user clicks on div, open the popupfunction myFunction() {
  var popup = document.getElementById("myPopup");
  popup.classList.toggle("show");
}</script>-->

<a name="awards"><h2>Awards</h2>
<ul>
<!-- <li> Tenure track offer for W2 (associate) professorship at <a href="https://www.usnews.com/education/best-global-universities/saarland-university-504282">Saarland University</a>, Germany 2022 </li>-->
<li> <a href="https://www.snf.ch/en/f6JZyI4uQ1mNeq3J/funding/discontinued-funding-schemes/early-postdoc-mobility"> Early Postdoc Mobility</a> grant from <a href="https://www.snf.ch/en">SNSF</a> for <i> Bridging the gap between local and global optimization in machine learning</i>, 20. <b>Outputs:</b> <a href="#4"> [4]</a>, <a href="#6"> [6]</a> and <a href="#9"> [9]</a> </li>
<li> Reviewer awards for my service to ICML 22, NeurIPS 20 and ICML 19. 
<li> Best poster award in deep learning workshop of <a href="https://learning-systems.org"> Max Planck ETH center for learning systems</a> 2016.
</ul>

<a name="talk"><h2>Talks</h2></a>
<ul> 
<li>Beyond Theoretical Mean-field Neural Networks at ISL Colloquium, Stanford University, July 23 (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/raw/main/slides/stanford.key">slides</a>) </li>
<li>Data representation in deep random neural networks at ML tea talks, MIT, March 23 (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/raw/main/slides/mit-tea-talk.key">slides</a>)</li>
<li>The power of depth in random neural networks at alg-ml-reading-group, Princeton University, April 22</li>
<li> Representations in Random Deep Neural Networks at Winter Seminar Series, Sharif University 22(<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/slides/wss22.pdf">slides</a>)</li>
<li> Batch normalization orthogonalizes representations in deep random neural networks, spotlight at NeurIPs 21 (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/slides/neurips21.pdf">slides</a>)</li>  
<li> Escaping Saddles with Stochastic Gradients at ICML 18 (<a href="https://github.com/hadidaneshmand/hadidaneshmand.github.io/blob/main/slides/icml18">slides</a>)</li>
</ul>
<a name="service"><h2>Academic Services</h2>
<ul> 
<li> Area chair for NeurIPS 23 </li>
<li> An organizer of <a href="https://sites.google.com/view/optml/home"> TILOS & OPTML++ seminars at MIT </a> </li>
<li> Conference reviewer for ICML, NeurIPs, ICLR and AISTATS</li>
<li> Journal reviewer for Data Mining and Knowledge Discovery, Neurocomputing, TPAMI, and TSIPN </li> 
</ul>

<h2> <a name="mentorship"> Mentorship </a></h2> 
I am privileged to work with excellent students who are potential future leaders of machine learning research. 
<ul>
<li> Amir Joudaki, Ph.D. student at ETH Zurich (since 20)</li>
<li> Jonas Kohler, former Ph.D. student at ETH  (18-20), joined Meta</li> 
<li> Antonio Orvieto, Ph.D. student at ETH Zurich (20-21) </li>
<li> Kwangjun Ahn, Ph.D. student at MIT (since 23)</li>
<li> Ashkan Soleymani, Ph.D. student at MIT (since 23) </li>  
<li> Peiyuan Zhang, Master's student at ETH (19-21), joined Yale University for Ph.D. </li>
<li> Leonard Adolphs, Master's student at ETH (18-19), joined ETH Zurich for Ph.D. </li> 
<li> Alexandru Meterez, Master's student at ETH (since 22) </li> 
<li> Flowers Alec Massimo, Master's student at ETH (since 23) </li> 
<li> Alexandre Bense, Master's student at ETH (22) </li> 
<li> Alireza Amani, Intern at ETH (18), joined London Business School for Ph.D. </li>
</ul>
</body>
</html>
